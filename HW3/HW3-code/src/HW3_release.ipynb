{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"7H420KxmCjKH","executionInfo":{"status":"ok","timestamp":1684796091494,"user_tz":420,"elapsed":2,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"outputs":[],"source":["import os\n","import sys"]},{"cell_type":"code","source":["# To add your own Drive Run this cell.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"BCDUFCh7Fd-n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684796407643,"user_tz":420,"elapsed":25971,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"6c4d8ded-c0e5-4c2a-f925-191d6706dcd6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Please append your own directory after â€˜/content/drive/My Drive/'\n","### ========== TODO : START ========== ###\n","sys.path += ['/content/drive/MyDrive/HW3/HW3-code']\n","### ========== TODO : END ========== ###"],"metadata":{"id":"nQXiXrbaF3NK","executionInfo":{"status":"ok","timestamp":1684796428909,"user_tz":420,"elapsed":659,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Author      : Yi-Chieh Wu, Sriram Sankararman\n","Description : Twitter\n","\"\"\"\n","\n","from string import punctuation\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# !!! MAKE SURE TO USE LinearSVC.decision_function(X), NOT LinearSVC.predict(X) !!!\n","# (this makes ''continuous-valued'' predictions)\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics"],"metadata":{"id":"7_OLupUPC2U3","executionInfo":{"status":"ok","timestamp":1684796431689,"user_tz":420,"elapsed":937,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Problem 3: Twitter Analysis Using SVM"],"metadata":{"id":"47L2XVzBX6c5"}},{"cell_type":"code","source":["######################################################################\n","# functions -- input/output\n","######################################################################\n","\n","def read_vector_file(fname):\n","    \"\"\"\n","    Reads and returns a vector from a file.\n","\n","    Parameters\n","    --------------------\n","        fname  -- string, filename\n","\n","    Returns\n","    --------------------\n","        labels -- numpy array of shape (n,)\n","                    n is the number of non-blank lines in the text file\n","    \"\"\"\n","    return np.genfromtxt(fname)\n","\n","\n","def write_label_answer(vec, outfile):\n","    \"\"\"\n","    Writes your label vector to the given file.\n","\n","    Parameters\n","    --------------------\n","        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n","        outfile -- string, output filename\n","    \"\"\"\n","\n","    # for this project, you should predict 70 labels\n","    if(vec.shape[0] != 70):\n","        print(\"Error - output vector should have 70 rows.\")\n","        print(\"Aborting write.\")\n","        return\n","\n","    np.savetxt(outfile, vec)\n","    "],"metadata":{"id":"9Z8E5YL0CzWe","executionInfo":{"status":"ok","timestamp":1684796434403,"user_tz":420,"elapsed":166,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["######################################################################\n","# functions -- feature extraction\n","######################################################################\n","\n","def extract_words(input_string):\n","    \"\"\"\n","    Processes the input_string, separating it into \"words\" based on the presence\n","    of spaces, and separating punctuation marks into their own words.\n","\n","    Parameters\n","    --------------------\n","        input_string -- string of characters\n","\n","    Returns\n","    --------------------\n","        words        -- list of lowercase \"words\"\n","    \"\"\"\n","\n","    for c in punctuation :\n","        input_string = input_string.replace(c, ' ' + c + ' ')\n","    return input_string.lower().split()\n","\n","\n","def extract_dictionary(infile):\n","    \"\"\"\n","    Given a filename, reads the text file and builds a dictionary of unique\n","    words/punctuations.\n","\n","    Parameters\n","    --------------------\n","        infile    -- string, filename\n","\n","    Returns\n","    --------------------\n","        word_list -- dictionary, (key, value) pairs are (word, index)\n","    \"\"\"\n","\n","    word_list = {}\n","    idx = 0\n","    with open(infile, 'r') as fid :\n","        # process each line to populate word_list\n","        for input_string in fid:\n","            words = extract_words(input_string)\n","            for word in words:\n","                if word not in word_list:\n","                    word_list[word] = idx\n","                    idx += 1\n","    return word_list\n","\n","\n","def extract_feature_vectors(infile, word_list):\n","    \"\"\"\n","    Produces a bag-of-words representation of a text file specified by the\n","    filename infile based on the dictionary word_list.\n","\n","    Parameters\n","    --------------------\n","        infile         -- string, filename\n","        word_list      -- dictionary, (key, value) pairs are (word, index)\n","\n","    Returns\n","    --------------------\n","        feature_matrix -- numpy array of shape (n,d)\n","                          boolean (0,1) array indicating word presence in a string\n","                            n is the number of non-blank lines in the text file\n","                            d is the number of unique words in the text file\n","    \"\"\"\n","\n","    num_lines = sum(1 for line in open(infile,'r'))\n","    num_words = len(word_list)\n","    feature_matrix = np.zeros((num_lines, num_words))\n","\n","    with open(infile, 'r') as fid :\n","        # process each line to populate feature_matrix\n","        for i, input_string in enumerate(fid):\n","            words = extract_words(input_string)\n","            for word in words:\n","                feature_matrix[i, word_list[word]] = 1.0\n","\n","    return feature_matrix"],"metadata":{"id":"i67aTAmrGGHi","executionInfo":{"status":"ok","timestamp":1684796455676,"user_tz":420,"elapsed":257,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["######################################################################\n","# functions -- evaluation\n","######################################################################\n","\n","def performance(y_true, y_pred, metric=\"accuracy\"):\n","    \"\"\"\n","    Calculates the performance metric based on the agreement between the\n","    true labels and the predicted labels.\n","\n","    Parameters\n","    --------------------\n","        y_true -- numpy array of shape (n,), known labels\n","        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n","        metric -- string, option used to select the performance measure\n","                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n","                           'sensitivity', 'specificity'\n","\n","    Returns\n","    --------------------\n","        score  -- float, performance score\n","    \"\"\"\n","    # map continuous-valued predictions to binary labels\n","    y_label = np.sign(y_pred)\n","    y_label[y_label==0] = 1\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1a: compute classifier performance\n","    if metric == \"accuracy\":\n","      score = metrics.accuracy_score(y_true, y_label)\n","    elif metric == \"f1-score\":\n","      score = metrics.f1_score(y_true, y_label)\n","    elif metric == \"auroc\":\n","      score = metrics.roc_auc_score(y_true, y_pred)\n","    elif metric == \"precision\":\n","      score = metrics.precision_score(y_true, y_label)\n","    elif metric == \"sensitivity\":\n","      score = metrics.recall_score(y_true, y_label)\n","    elif metric == \"specificity\":\n","      tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_label).ravel()\n","      score = tn / (tn + fp)\n","    else:\n","      raise ValueError(\"Enter a valid metric\")\n","\n","    return score\n","    ### ========== TODO : END ========== ###\n","\n","\n","def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n","    \"\"\"\n","    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n","    Trains classifier on k-1 folds and tests on the remaining fold.\n","    Calculates the k-fold cross-validation performance metric for classifier\n","    by averaging the performance across folds.\n","\n","    Parameters\n","    --------------------\n","        clf    -- classifier (instance of LinearSVC)\n","        X      -- numpy array of shape (n,d), feature vectors\n","                    n = number of examples\n","                    d = number of features\n","        y      -- numpy array of shape (n,), binary labels {1,-1}\n","        kf     -- model_selection.StratifiedKFold\n","        metric -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        score   -- float, average cross-validation performance across k folds\n","    \"\"\"\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1b: compute average cross-validation performance\n","    scores = []\n","\n","    for train_index, test_index in kf.split(X, y):\n","      X_train, X_test = X[train_index], X[test_index]\n","      y_train, y_test = y[train_index], y[test_index]\n","\n","      clf.fit(X_train, y_train)\n","      y_pred = clf.decision_function(X_test)\n","\n","      score = performance(y_test, y_pred, metric=metric)\n","      scores.append(score)\n","\n","    return np.mean(scores)\n","    ### ========== TODO : END ========== ###\n","\n","\n","def select_param_linear(X, y, kf, metric=\"accuracy\"):\n","    \"\"\"\n","    Sweeps different settings for the hyperparameter of a linear SVM,\n","    calculating the k-fold CV performance for each setting, then selecting the\n","    hyperparameter that 'maximize' the average k-fold CV performance.\n","\n","    Parameters\n","    --------------------\n","        X      -- numpy array of shape (n,d), feature vectors\n","                    n = number of examples\n","                    d = number of features\n","        y      -- numpy array of shape (n,), binary labels {1,-1}\n","        kf     -- model_selection.StratifiedKFold\n","        metric -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        C -- float, optimal parameter value for linear SVM\n","    \"\"\"\n","\n","    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n","    C_range = 10.0 ** np.arange(-3, 3)\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1c: select optimal hyperparameter using cross-validation  \n","    best_C = None\n","    best_performance = -float('inf')\n","\n","    for c in C_range:\n","      clf = LinearSVC(loss='hinge', random_state=0, C=c)\n","      avg_performance = cv_performance(clf, X, y, kf, metric=metric)\n","\n","      if avg_performance > best_performance:\n","        best_performance = avg_performance\n","        best_C = c\n","\n","    return best_C\n","    ### ========== TODO : END ========== ###\n","\n","\n","def performance_test(clf, X, y, metric=\"accuracy\"):\n","    \"\"\"\n","    Estimates the performance of the classifier.\n","\n","    Parameters\n","    --------------------\n","        clf          -- classifier (instance of LinearSVC)\n","                          [already fit to data]\n","        X            -- numpy array of shape (n,d), feature vectors of test set\n","                          n = number of examples\n","                          d = number of features\n","        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n","        metric       -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        score        -- float, classifier performance\n","    \"\"\"\n","\n","\n","    ### ========== TODO : START ========== ###\n","    # part 2b: return performance on test data under a metric.\n","    y_pred = clf.decision_function(X)\n","    score = performance(y, y_pred, metric=metric)\n","    return score\n","\n","    ### ========== TODO : END ========== ###"],"metadata":{"id":"-MvTxQPRGOOf","executionInfo":{"status":"ok","timestamp":1684797004260,"user_tz":420,"elapsed":432,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["######################################################################\n","# main\n","######################################################################\n","\n","def main() :\n","    np.random.seed(1234)\n","\n","    # read the tweets and its labels, change the following two lines to your own path.\n","    ### ========== TODO : START ========== ###\n","    file_path = '/content/drive/MyDrive/HW3/HW3-code/data/tweets.txt'\n","    label_path = '/content/drive/MyDrive/HW3/HW3-code/data/labels.txt'\n","    ### ========== TODO : END ========== ###\n","    dictionary = extract_dictionary(file_path)\n","    #print(len(dictionary))\n","    X = extract_feature_vectors(file_path, dictionary)\n","    y = read_vector_file(label_path)\n","    # split data into training (training + cross-validation) and testing set\n","    X_train, X_test = X[:560], X[560:]\n","    y_train, y_test = y[:560], y[560:]\n","\n","    metric_list = [\"accuracy\", \"f1-score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1b: create stratified folds (5-fold CV)\n","    kf = StratifiedKFold(n_splits=5)\n","    # part 1c: for each metric, select optimal hyperparameter for linear SVM using CV\n","\n","    best_Cs = []\n","    for metric in metric_list:\n","      best_C = select_param_linear(X_train, y_train, kf, metric)\n","      print(best_C)\n","      best_Cs.append(best_C)\n","    # part 2a: train linear SVMs with selected hyperparameters\n","    clfs = []\n","    for best_C in best_Cs:\n","      clf = LinearSVC(loss='hinge', random_state=0, C=best_C)\n","      clf.fit(X_train, y_train)\n","      clfs.append(clf)\n","    # part 2b: test the performance of your classifiers.\n","    for metric, clf in zip(metric_list, clfs):\n","      score = performance_test(clf, X_test, y_test, metric)\n","      print(f\"Performance on test data for {metric}: {score}\")\n","    ### ========== TODO : END ========== ###\n","\n","\n","if __name__ == \"__main__\" :\n","    main()"],"metadata":{"id":"zMIQRGpYErVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684797186479,"user_tz":420,"elapsed":3928,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"b6682ffb-fc6e-422d-b582-b70a089c3c65"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear SVM Hyperparameter Selection based on accuracy:\n","1.0\n","Linear SVM Hyperparameter Selection based on f1-score:\n","1.0\n","Linear SVM Hyperparameter Selection based on auroc:\n","1.0\n","Linear SVM Hyperparameter Selection based on precision:\n","10.0\n","Linear SVM Hyperparameter Selection based on sensitivity:\n","0.001\n","Linear SVM Hyperparameter Selection based on specificity:\n","1.0\n","Performance on test data for accuracy: 0.7428571428571429\n","Performance on test data for f1-score: 0.47058823529411764\n","Performance on test data for auroc: 0.7424684159378038\n","Performance on test data for precision: 0.6363636363636364\n","Performance on test data for sensitivity: 1.0\n","Performance on test data for specificity: 0.8979591836734694\n"]}]},{"cell_type":"markdown","source":["# Problem 4: Boosting vs. Decision Tree"],"metadata":{"id":"_W-_mjX0JMes"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score, train_test_split"],"metadata":{"id":"0uzCdPTkOQSY","executionInfo":{"status":"ok","timestamp":1684788162289,"user_tz":420,"elapsed":663,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class Data :\n","    \n","    def __init__(self) :\n","        \"\"\"\n","        Data class.\n","        \n","        Attributes\n","        --------------------\n","            X -- numpy array of shape (n,d), features\n","            y -- numpy array of shape (n,), targets\n","        \"\"\"\n","                \n","        # n = number of examples, d = dimensionality\n","        self.X = None\n","        self.y = None\n","        \n","        self.Xnames = None\n","        self.yname = None\n","    \n","    def load(self, filename, header=0, predict_col=-1) :\n","        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n","        \n","        # determine filename\n","        f = filename\n","        \n","        # load data\n","        with open(f, 'r') as fid :\n","            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n","        \n","        # separate features and labels\n","        if predict_col is None :\n","            self.X = data[:,:]\n","            self.y = None\n","        else :\n","            if data.ndim > 1 :\n","                self.X = np.delete(data, predict_col, axis=1)\n","                self.y = data[:,predict_col]\n","            else :\n","                self.X = None\n","                self.y = data[:]\n","        \n","        # load feature and label names\n","        if header != 0:\n","            with open(f, 'r') as fid :\n","                header = fid.readline().rstrip().split(\",\")\n","                \n","            if predict_col is None :\n","                self.Xnames = header[:]\n","                self.yname = None\n","            else :\n","                if len(header) > 1 :\n","                    self.Xnames = np.delete(header, predict_col)\n","                    self.yname = header[predict_col]\n","                else :\n","                    self.Xnames = None\n","                    self.yname = header[0]\n","        else:\n","            self.Xnames = None\n","            self.yname = None\n","\n","\n","# helper functions\n","def load_data(filename, header=0, predict_col=-1) :\n","    \"\"\"Load csv file into Data class.\"\"\"\n","    data = Data()\n","    data.load(filename, header=header, predict_col=predict_col)\n","    return data"],"metadata":{"id":"DVxef2sxOmVI","executionInfo":{"status":"ok","timestamp":1684788164995,"user_tz":420,"elapsed":3,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Change the path to your own data directory\n","### ========== TODO : START ========== ###\n","titanic = load_data(\"/content/drive/MyDrive/HW3/HW3-code/data/titanic_train.csv\", header=1, predict_col=0)\n","### ========== TODO : END ========== ###\n","X = titanic.X; Xnames = titanic.Xnames\n","y = titanic.y; yname = titanic.yname\n","n,d = X.shape  # n = number of examples, d =  number of features"],"metadata":{"id":"_Zcf4WVqJSpe","executionInfo":{"status":"ok","timestamp":1684788168451,"user_tz":420,"elapsed":945,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def error(clf, X, y, ntrials=100, test_size=0.2) :\n","    \"\"\"\n","    Computes the classifier error over a random split of the data,\n","    averaged over ntrials runs.\n","\n","    Parameters\n","    --------------------\n","        clf         -- classifier\n","        X           -- numpy array of shape (n,d), features values\n","        y           -- numpy array of shape (n,), target classes\n","        ntrials     -- integer, number of trials\n","        test_size   -- proportion of data used for evaluation\n","\n","    Returns\n","    --------------------\n","        train_error -- float, training error\n","        test_error  -- float, test error\n","    \"\"\"\n","\n","    train_error = 0\n","    test_error = 0\n","\n","    train_scores = []; \n","    test_scores = [];\n","    for i in range(ntrials):\n","        xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size = test_size, random_state=i)\n","        clf.fit(xtrain, ytrain)\n","\n","        ypred = clf.predict (xtrain)\n","        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n","        train_scores.append (err)\n","\n","        ypred = clf.predict (xtest)\n","        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n","        test_scores.append (err)\n","\n","    train_error =  np.mean (train_scores)\n","    test_error = np.mean (test_scores)\n","    return train_error, test_error\n"],"metadata":{"id":"3Ta7XHRWQGNo","executionInfo":{"status":"ok","timestamp":1684788239232,"user_tz":420,"elapsed":539,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["### ========== TODO : START ========== ###\n","# Part 4(a): Implement the decision tree classifier and report the training error.\n","print('Classifying using Decision Tree...')\n","\n","# Create a decision tree classifier\n","clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n","\n","# Train the classifier on the whole dataset\n","clf.fit(X, y)\n","\n","\n","# Compute the training error\n","err = error(clf, X, y)\n","\n","# Print the training error\n","print(f\"Training Error: {err[0]}\")\n","print(f\"Test Error: {err[1]}\")\n","### ========== TODO : END ========== ###"],"metadata":{"id":"W8-U3un5PjGq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684788249028,"user_tz":420,"elapsed":1110,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"0916ae28-676d-4b12-e24f-b4e933e34882"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying using Decision Tree...\n","Training Error: 0.011528998242530775\n","Test Error: 0.24104895104895108\n"]}]},{"cell_type":"code","source":["### ========== TODO : START ========== ###\n","# Part 4(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n","print('Classifying using Random Forest...')\n","max_samples_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n","\n","# Initialize variables to track the best setting and corresponding error\n","best_max_samples = None\n","best_test_error = float('inf')\n","best_training_error = None\n","\n","# Loop over the max_samples values\n","for max_samples in max_samples_values:\n","    #print(f\"Max Samples: {max_samples}\")\n","\n","    # Create a random forest classifier\n","    clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=max_samples)\n","\n","    # Train the classifier on the whole dataset\n","    clf.fit(X, y)\n","\n","    # Compute the training and test errors using the error function\n","    training_error, test_error = error(clf, X, y)\n","\n","    # Print the training and test errors\n","    #print(f\"Training Error: {training_error}\")\n","    #print(f\"Test Error: {test_error}\")\n","\n","    # Update the best setting and error if necessary\n","    if test_error < best_test_error:\n","        best_max_samples = max_samples\n","        best_test_error = test_error\n","        best_training_error = training_error\n","\n","# Print the best setting and corresponding error\n","print(f\"Best Max Samples: {best_max_samples}\")\n","print(f\"Test Error: {best_test_error}\")\n","print(f\"Training Error: {best_training_error}\")\n","### ========== TODO : END ========== ###"],"metadata":{"id":"_x_PevK8Q4dx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684788644854,"user_tz":420,"elapsed":182355,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"66571154-050d-4195-b7ad-f3344fee2a7f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying using Random Forest...\n","Best Max Samples: 0.3\n","Test Error: 0.1874825174825175\n","Training Error: 0.09427065026362039\n"]}]},{"cell_type":"code","source":["### ========== TODO : START ========== ###\n","# Part 4(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n","print('Classifying using Random Forest...')\n","max_features_values = list(range(1, 8))\n","\n","# Initialize variables to track the best setting and corresponding error\n","best_max_features = None\n","best_test_error = float('inf')\n","best_training_error = None\n","\n","# Loop over the max_features values\n","for max_features in max_features_values:\n","    #print(f\"Max Features: {max_features}\")\n","\n","    # Create a random forest classifier with the best max_samples value from Part b\n","    clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=best_max_samples, max_features=max_features)\n","\n","    # Train the classifier on the whole dataset\n","    clf.fit(X, y)\n","\n","    # Compute the training and test errors using the error function\n","    training_error, test_error = error(clf, X, y)\n","\n","    # Print the training and test errors\n","    #print(f\"Training Error: {training_error}\")\n","    #print(f\"Test Error: {test_error}\")\n","\n","    # Update the best setting and error if necessary\n","    if test_error < best_test_error:\n","        best_max_features = max_features\n","        best_test_error = test_error\n","        best_training_error = training_error\n","\n","# Print the best setting and corresponding error\n","print(f\"Best Max Features: {best_max_features}\")\n","print(f\"Test Error: {best_test_error}\")\n","print(f\"Training Error: {best_training_error}\")\n","### ========== TODO : END ========== ###"],"metadata":{"id":"ZFUyPTPwT53v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684788818748,"user_tz":420,"elapsed":159517,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"b4244f14-2c12-4c2d-92bb-12c22ee34a3b"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying using Random Forest...\n","Best Max Features: 3\n","Test Error: 0.18678321678321677\n","Training Error: 0.09481546572934976\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","x1 = [0, 1, 3, -2, -1, 10, 12, -7, -3, 5]\n","x2 = [5, 4, 7, 1, 13, 3, 7, -1, 12, 9]\n","y = [-1, -1, 1, 1, -1, -1, 1, -1, 1, 1]\n","\n","def error(x, y, j, weights):\n","  err = 0\n","  for i in range(10):\n","    label = np.sign(x[i] - j)\n","\n","    if label != y[i]:\n","      err += weights[i]\n","  \n","  return err\n","  \n","w1 = 10 * [0.1]\n","w2 = [0.0625, 0.0625, 0.0625, 0.25, 0.25, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]\n","\n","def find_optimal(data, w):\n","  min_error = float(\"inf\")\n","  best_i = None\n","\n","  for i in range(-100, 100):\n","    if i not in data:\n","      err = error(data, y, i, w)\n","\n","      if err < min_error:\n","        min_error = err\n","        best_i = i\n","  \n","  return min_error, best_i\n","\n","j1 = find_optimal(x1, w1)[1]\n","j2 = find_optimal(x2, w1)[1]\n","j1_p = find_optimal(x1, w2)[1]\n","j2_p = find_optimal(x2, w2)[1]\n","\n","print(f\"Optimal j1: {j1}\")\n","print(f\"Optimal j2: {j2}\")\n","print(f\"Optimal j1': {j1_p}\")\n","print(f\"Optimal j2: {j2_p}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQcb4oqMuVIy","executionInfo":{"status":"ok","timestamp":1684811011872,"user_tz":420,"elapsed":446,"user":{"displayName":"TYLER STOVSKY","userId":"17922680748847695326"}},"outputId":"fda955b6-972c-44b2-e78d-bcb796ac3f31"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal j1: 2\n","Optimal j2: 6\n","Optimal j1': 2\n","Optimal j2: 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ALua0mqESYJJ"},"execution_count":null,"outputs":[]}]}